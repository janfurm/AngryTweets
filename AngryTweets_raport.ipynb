{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raport z analizy Tweet'ów\n",
    "\n",
    "Celem [projektu](https://inclass.kaggle.com/c/angry-tweets) było zbudowanie klasyfikatora, pozwalającego na sklasyfikowanie Tweet'a jako pozytywnego, nutralnego, lub negatywnego. Klasyfikator zgodnie z założeniem został zbudowany w oparciu o [Python](https://www.continuum.io/downloads)'a oraz [scikit-learn](http://scikit-learn.org/stable/).\n",
    "\n",
    "Niniejszy raport stanowi zwięzłe podsumowanie przeprowadzonych działań."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wykorzystane bibilioteki\n",
    "\n",
    "W projekcie wykorzystano następujące biblioteki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Przygotowanie Tweet'ów\n",
    "\n",
    "Przed przystąpieniem do budowy klasyfikatora konieczne jest odpowiednie przygotowanie Tweet'ów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Zdefiniowanie zmiennych\n",
    "\n",
    "W poniższych zmiennych zostały zdefiniowane wyrażenia regularne oraz lista słów, które zostanły wykorzystane w dalszej części do przygotowania Tweet'ów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
    "RE_NUM = re.compile(\"[0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
    "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
    "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
    "            \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
    "            \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
    "            \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "            \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
    "            \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\"whom\", \"with\", \"would\", \"you\",\n",
    "            \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "            \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
    "            \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\",\n",
    "             \"over\", \"same\", \"too\", \"under\", \"why\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Funkcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja *normalize* umożliwia na zamianę encji html na poprawne znaki tekstowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = text.strip().lower()\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&pound;', u'£')\n",
    "    text = text.replace('&euro;', u'€')\n",
    "    text = text.replace('&copy;', u'©')\n",
    "    text = text.replace('&reg;', u'®')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja *tokenize* służy do tokenizacji, czyli wyodrębnianiu z tekstu poszczególnych słów i znaków interpunkcyjnych - tokenów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja *stem* służy do sprowadzania słów do ich podstawowej formy (*word stem*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(stem))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja *clean_tokens* służy do usunięcia tokenów nie niosących dodatkowej treści tj. adresy url, hashtag'ów, odwołań do użtkowników, czy znaków interpunkcyjnych. Funkcja korzysta z wcześniej zdefiniowanych wyrażeń regularnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        match = re.search(RE_HTTP, token) or re.search(RE_HASHTAG, token)\n",
    "        match2 = (len(token) < 2) #or (token in string.punctuation)\n",
    "        if (match is not None) or match2:\n",
    "            del tokens[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja *remove_stopwords* pozwala na usunięcie tokenów, które znajdują się na liście *stopwords* - liście słów, które analogicznie jak powyżej nie stanowią istotnej treści."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(words, stopwords):\n",
    "    words_clean = [word for word in words if word not in stopwords]\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadaniem funkcji *preprocess* jest kompleksowe przygotowanie tweet'ów do dalszej analizy. Funkcja jest kompozycją wyżej przedstawionych kroków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tweets):\n",
    "    tweets_tokenized = []\n",
    "\n",
    "    for index, row in tweets.iterrows():\n",
    "        tweet = row[-1]\n",
    "\n",
    "        if not pd.isnull(tweet):\n",
    "            tweet = normalize(tweet)\n",
    "            tweet = tokenize(tweet)\n",
    "            clean_tokens(tweet)\n",
    "            tweet = remove_stopwords(tweet, stopwords)\n",
    "        else:\n",
    "            tweet = []\n",
    "\n",
    "        if len(tweets.columns) == 3:\n",
    "            tweets_tokenized.append([row[0], row[1], tweet])\n",
    "        elif len(tweets.columns) == 2:\n",
    "            tweets_tokenized.append([row[0], tweet])\n",
    "\n",
    "    tweets_tokenized_df = pd.DataFrame(tweets_tokenized)\n",
    "    #tweets_tokenized_df = tweets_tokenized_df.sample(frac=1).reset_index(drop=True)\n",
    "    return tweets_tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Przygotowanie zbioru uczącego\n",
    "\n",
    "W pliku *train.csv* znajduje się zbiór treningowy, zawierający identyfikator tweet'a, jego treść oraz przyporządkowaną mu kategorię. Należy zwrócić uwagę, iż w zbiorze znajdują się tweet'y nie zawierające treści - etykietowane jako *Not Avaible*. Zostało to uwzględnione podczas wczytywania, ponieważ w innym wypadku etykieta zostałaby potraktowana jako treść tweet'a.\n",
    "\n",
    "Wczytany zbiór treningowy jest losowo mieszany, co jest istotne z punktu widzenia późniejszego jego podziału do walidacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"data/train.csv\", sep=',', header=0, na_values='Not Available')\n",
    "tweets_tokenized_df = preprocess(tweets)\n",
    "tweets_tokenized_df = tweets_tokenized_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Klasyfikator\n",
    "\n",
    "Przeprowadzona w ramach projektu analiza wykazała, że najlepsze rezultaty otrzymano stosując klasyfikator *MultinomialNB* oraz reprezentacji TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NBayes_tfidf_classifier(tweets_tokenized_df, perc):\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    data = tweets_tokenized_df.iloc[:,2].tolist()\n",
    "    category = tweets_tokenized_df.iloc[:,1].tolist()\n",
    "    num = int(round(len(data_tmp) * perc))\n",
    "    \n",
    "    data_tmp = []\n",
    "    for row in data:\n",
    "        if row == []:\n",
    "            data_tmp.append('')\n",
    "        else:\n",
    "            data_tmp.append(' '.join(row))\n",
    "\n",
    "    classifier = Pipeline([\n",
    "        ('count_vectorizer', CountVectorizer(ngram_range=(1,  2), max_df=0.8, min_df=8)),\n",
    "        ('tfidf_transformer', TfidfTransformer()),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    classifier.fit(data_tmp[:num], category[:num])\n",
    "\n",
    "    print(\"Train set size \", num)\n",
    "    print(\"Test set size \", len(data_tmp)-num)\n",
    "    from sklearn.metrics import classification_report\n",
    "    prediction = classifier.predict(data_tmp[:num])\n",
    "    print(\"Train data classification report:\")\n",
    "    print(classification_report(category[:num], prediction))\n",
    "\n",
    "    if len(data_tmp)-num > 0:\n",
    "        print(\"Test data classification report:\")\n",
    "        prediction2 = classifier.predict(data_tmp[num:])\n",
    "        print(classification_report(category[num:], prediction2))\n",
    "\n",
    "    print(\"Submission set predicting\")\n",
    "    tweets_test = pd.read_csv(\"data/test.csv\", sep=',', header=0, na_values='Not Available')\n",
    "    tweets_tokenized_df = preprocess(tweets_test)\n",
    "\n",
    "    data = tweets_tokenized_df.iloc[:,-1].tolist()\n",
    "    data_tmp = []\n",
    "    tt = 0;\n",
    "    for row in data:\n",
    "        if row == []:\n",
    "            data_tmp.append('')\n",
    "        else:\n",
    "            data_tmp.append(' '.join(row))\n",
    "\n",
    "    prediction3 = classifier.predict(data_tmp)\n",
    "\n",
    "    submission = pd.concat([tweets_tokenized_df.iloc[:, 0], pd.DataFrame(prediction3)], axis=1)\n",
    "    submission.columns = ['Id', 'Category']\n",
    "    submission.to_csv('submission.csv', sep=',', index=False, index_label=False)\n",
    "\n",
    "    print(\"Data saved to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatecznie, wywołanie funkcji ma postać:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NBayes_tfidf_classifier(tweets_tokenized_df, 0.7)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
